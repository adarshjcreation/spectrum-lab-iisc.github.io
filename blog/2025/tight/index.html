<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Role of the Data-Fidelity Term in Solving Linear Inverse Problems | </title> <meta name="author" content="Spectrum Lab"> <meta name="description" content=""> <meta name="keywords" content="IISc, Bengaluru, Indian Institute of Science, signal processing, machine learing, computational imaging, reserach, generative modelling, artificial intelligence"> <meta property="og:site_name" content=""> <meta property="og:type" content="article"> <meta property="og:title" content=" | The Role of the Data-Fidelity Term in Solving Linear Inverse Problems"> <meta property="og:url" content="https://spectrum-lab-iisc.github.io/blog/2025/tight/"> <meta property="og:description" content=""> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="The Role of the Data-Fidelity Term in Solving Linear Inverse Problems"> <meta name="twitter:description" content=""> <meta name="twitter:site" content="@SpectrumLabIISc"> <meta name="twitter:creator" content="@SpectrumLabIISc"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Spectrum Lab"
        },
        "url": "https://spectrum-lab-iisc.github.io/blog/2025/tight/",
        "@type": "BlogPosting",
        "description": "",
        "headline": "The Role of the Data-Fidelity Term in Solving Linear Inverse Problems",
        
        "sameAs": ["https://scholar.google.com/citations?user=1g1i1B4AAAAJ", "https://github.com/spectrum-lab-iisc", "https://twitter.com/SpectrumLabIISc"],
        
        "name": "Spectrum Lab",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" type="text/css" href="https://use.typekit.net/pzd6qvr.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://spectrum-lab-iisc.github.io/blog/2025/tight/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11/swiper-bundle.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Role of the Data-Fidelity Term in Solving Linear Inverse Problems",
            "description": "",
            "published": "August 06, 2025",
            "updatedDate": "July 12, 2025",
            "postAuthor": "Abijith J. Kamath",
            "authors": [
              
              {
                "author": "Abijith J. Kamath",
                "authorURL": "https://kamath-abhijith.github.io",
                "affiliations": [
                  {
                    "name": "Indian Institute of Science",
                    "url": ""
                  }
                ]
              }
              
            ],
            "doi": "10.1137/23M1625846",
            "url": "https://epubs.siam.org/doi/epdf/10.1137/23M1625846",
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img class="navbar spectrum-logo-light" src="/assets/img/SpectrumLab/SpectrumLogo%20Black.png" alt=""> <img class="navbar spectrum-logo-dark" src="/assets/img/SpectrumLab/SpectrumLogo%20White.png" alt=""> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Research </a> </li> <li class="nav-item "> <a class="nav-link" href="/opportunities/">Opportunities </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/alumni/">Alumni </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/recognition/">Recognition </a> </li> <li class="nav-item "> <a class="nav-link" href="/contactus/">Contact </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Role of the Data-Fidelity Term in Solving Linear Inverse Problems</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#the-data-fidelity-term-in-linear-inverse-problems">The Data-Fidelity Term in Linear Inverse Problems</a></li> <li class="toc-entry toc-h2"><a href="#introduction-to-frames">Introduction to Frames</a></li> <li class="toc-entry toc-h2"><a href="#the-back-projection-loss-as-a-distance-minimization">The Back-Projection Loss as a Distance Minimization</a></li> <li class="toc-entry toc-h2"> <a href="#back-projection-and-tight-frames-in-compressed-sensing">Back-Projection and Tight Frames in Compressed Sensing</a> <ul> <li class="toc-entry toc-h3"><a href="#minimum-mse-recovery-with-an-oracle">Minimum MSE Recovery with an Oracle</a></li> <li class="toc-entry toc-h3"><a href="#the-back-projection-bridge">The Back-Projection Bridge</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#a-wiener-filter-like-update-for-image-deconvolution">A Wiener-Filter-like Update for Image Deconvolution</a></li> <li class="toc-entry toc-h2"><a href="#solving-inverse-problems-with-the-method-of-alternating-proximations">Solving Inverse Problems with the Method of Alternating Proximations</a></li> </ul> </nav> </d-contents> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0 blog-ready"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tight/alt_prox.jpg" sizes="95vw"></source> <img src="/assets/img/tight/alt_prox.jpg" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="the-data-fidelity-term-in-linear-inverse-problems">The Data-Fidelity Term in Linear Inverse Problems</h2> <p class="text-justify">In some of our recent works, we investigate the role of the data-fidelity term in solving linear inverse problems. Most often in linear inverse problems, the $\ell_2$-norm is used to compute the error between the measurements and the measurement model. Consider the measurement model:</p> <div style="text-align: center;"> $y = Ax + w.$ <br><br> </div> <p class="text-justify">Here, $A$ is the forward operator (or sensing matrix) that describes the measurement process, and $w$ represents measurement noise. Since $A$ is often ill-conditioned or rectangular (with fewer measurements than signal dimensions), the problem is ill-posed and requires regularization to find a meaningful solution.</p> <p class="text-justify">The standard approach is to solve an optimization problem with the the $\ell_2$-norm as the data-fidelity metric:</p> <div style="text-align: center;"> $\min_{x} \frac{1}{2} \|Ax - y\|_2^2 + \lambda g(x)$ <br><br> </div> <p class="text-justify">Although this choice has statistical backing, i.e., the $\ell_2$-norm is a direct consequence of modelling additive noise as a Gaussian random variable in maximum a posteriori estimation, it is often not the best choice in practice <d-cite key="gribonval2021bayesian"></d-cite>.</p> <h2 id="introduction-to-frames">Introduction to Frames</h2> <p class="text-justify">In linear algebra, a basis for a vector space is a set of linearly independent vectors that span the space. This means any vector in the space can be written as a <em>unique</em> linear combination of the basis vectors. Frames are a generalization of bases that provide more flexibility. A frame is a set of vectors that also spans the space but is not required to be linearly independent. This redundancy can be highly beneficial for robustness to noise and errors.</p> <p class="text-justify">Formally, a set of $N$ vectors $\lbrace v_i \in \mathbb{R}^n \rbrace_{i=1}^N$ is said to constitute a <strong>frame</strong> for $\mathbb{R}^n$ if there exist constants $0 &lt; \alpha \le \beta &lt; \infty$ such that for any vector $x \in \mathbb{R}^n$, the following condition holds:</p> <div style="text-align: center;"> $\alpha \|x\|_2^2 \le \sum_{i=1}^{N} |\langle x, v_i \rangle|^2 \le \beta \|x\|_2^2$ <br><br> </div> <p>The constants $\alpha$ and $\beta$ are called the frame bounds.</p> <p class="text-justify">If the frame bounds are equal, $\alpha = \beta$, the frame is called a <strong>tight frame</strong>. If $\alpha = \beta = 1$, it is a <strong>Parseval tight frame</strong>. Tight frames share some of the desirable properties of orthonormal bases, making them particularly useful in areas like compressed sensing.</p> <h2 id="the-back-projection-loss-as-a-distance-minimization">The Back-Projection Loss as a Distance Minimization</h2> <p class="text-justify">The back-projection loss is defined as the squared distance from a candidate solution $x$ to the solution space of the measurement model. Let’s define the solution space (in the noiseless case) as the affine subspace $C = \lbrace z \in \mathbb{R}^n : Az = y\rbrace$. This set contains all possible signals that are perfectly consistent with the measurements $y$.</p> <p>The squared Euclidean distance from a point $x$ to this set is given by:</p> <div style="text-align: center;"> $d_C^2(x) = \min_{z \in C} \|x - z\|_2^2$ <br><br> </div> <p class="text-justify">The point in $C$ closest to $x$ is the orthogonal projection of $x$ onto $C$, denoted as $\text{proj}_C(x)$. For a full row-rank matrix $A$, this projection is given by:</p> <div style="text-align: center;"> $\text{proj}_C(x) = x - A^{\dagger}(Ax - y)$ <br><br> </div> <p class="text-justify">where $A^{\dagger} = A^T(AA^T)^{-1}$ is the Moore-Penrose pseudoinverse of $A$. Therefore, the squared distance is:</p> <div style="text-align: center;"> $d_C^2(x) = \|x - \text{proj}_C(x)\|_2^2 = \|A^{\dagger}(Ax - y)\|_2^2$ <br><br> </div> <p class="text-justify">The back-projection loss is precisely this squared distance. The optimization problem then becomes:</p> <div style="text-align: center;"> $\min_{x} \frac{1}{2} \|A^{\dagger}(y - Ax)\|_2^2 + \lambda g(x)$ <br><br> </div> <p class="text-justify">This formulation seeks a solution that is not only regularized by $g(x)$ but is also minimally distant from the space of all signals that could have produced the measurements. Such a choice has been shown to provide superior reconstruction accuracy for inverse problems in imaging <d-cite key="tirer2021convergence"></d-cite>, compressive sensing <d-cite key="nareddy2024tight"></d-cite>, and finite-rate-of-innovation signal reconstruction <d-cite key="kamath2025deepfri"></d-cite>.</p> <h2 id="back-projection-and-tight-frames-in-compressed-sensing">Back-Projection and Tight Frames in Compressed Sensing</h2> <p class="text-justify">In compressed sensing (CS), the goal is to recover a sparse signal from a few measurements. The choice of the sensing matrix $A$ is critical. While random Gaussian matrices are easy to construct and satisfy theoretical guarantees like the Restricted Isometry Property (RIP), <strong>tight-frame</strong> matrices are known to yield the minimum mean-squared error. A matrix $V$ is a Parseval tight frame if $VV^\top = I$.</p> <h3 id="minimum-mse-recovery-with-an-oracle">Minimum MSE Recovery with an Oracle</h3> <p class="text-justify">To understand why tight frames are optimal, consider an idealized scenario where an “oracle” tells us the exact locations (the support, denoted by $S$) of the non-zero entries in the sparse signal $x$. The recovery problem then simplifies to finding the <em>values</em> of these non-zero coefficients.</p> <p class="text-justify">Let $x_S$ be the vector of non-zero values of $x$ and $A_S$ be the submatrix of $A$ containing only the columns indexed by the support $S$. The measurement model becomes:</p> <div style="text-align: center;"> $y = A_S x_S + w$ <br><br> </div> <p class="text-justify">This is a standard linear estimation problem. The best linear unbiased estimator for $x_S$ is the least-squares solution, which gives the recovered values $\hat{x}_S$:</p> <div style="text-align: center;"> $\hat{x}_S = (A_S^T A_S)^{-1} A_S^T y$. <br><br> </div> <p class="text-justify">The mean-squared error (MSE) of this estimate is directly related to the properties of the matrix $(A_S^T A_S)^{-1}$. The error is minimized when this matrix is well-conditioned. A <strong>tight-frame</strong> sensing matrix $A$ ensures that for any support $S$, the columns of $A_S$ are as close to orthogonal as possible, which makes $A_S^T A_S$ well-conditioned (close to the identity matrix). This minimizes the amplification of the noise $w$ during the recovery process, thus achieving the minimum possible MSE.</p> <h3 id="the-back-projection-bridge">The Back-Projection Bridge</h3> <p class="text-justify">The back-projection loss provides a remarkable bridge between the convenience of random matrices and the optimality of tight frames. Consider the standard CS problem formulation with a non-tight sensing matrix $A$ (e.g., Gaussian) and the back-projection loss:</p> <div style="text-align: center;"> $\min_{x} \|D^\top x\|_1 \quad \text{subject to} \quad \|A^{\dagger}(y - Ax)\|_2 \le \epsilon$, <br><br> </div> <p>where $D$ is a sparsifying dictionary. This is equivalent to solving the problem with a modified data-fidelity term $\Vert B(Ax-y)\Vert^2_2$ where $B=(AA^T)^{-1/2}$.</p> <p class="text-justify">This formulation is equivalent to solving the original problem with an <em>effective</em> sensing matrix $\tilde{A} = BA = (AA^T)^{-1/2}A$ and effective measurements $\tilde{y} = By$. The key insight is that this new sensing matrix $\tilde{A}$ is a <strong>Parseval tight frame</strong>, because:</p> <div style="text-align: center;"> $\tilde{A}\tilde{A}^T = (BA)(BA)^T = BAA^TB^T = (AA^T)^{-1/2}AA^T(AA^T)^{-1/2} = I$. <br><br> </div> <p class="text-justify">This means that by simply changing the data-fidelity loss to the back-projection loss, we can gain the performance benefits of a tight-frame sensing matrix without the difficulty of constructing one. This leads to improved recovery guarantees, especially when the number of measurements is low or the signal is less sparse.</p> <h2 id="a-wiener-filter-like-update-for-image-deconvolution">A Wiener-Filter-like Update for Image Deconvolution</h2> <p class="text-justify">When the forward model $A$ represents a convolution, such as in image deblurring, the back-projection loss has another elegant interpretation. In iterative optimization algorithms like the proximal gradient method, the update step involves computing the gradient of the data-fidelity term.</p> <p class="text-justify">For the standard least-squares loss, the gradient is $\nabla f_{LS}(x) = A^T(Ax-y)$. For the back-projection loss, the gradient is $\nabla f_{BP}(x) = A^T(AA^T)^{-1}(Ax-y)$.</p> <p class="text-justify">In practice, the matrix $AA^T$ can be ill-conditioned, so a regularized inverse is used. For a convolutional operator $H$, the gradient update becomes:</p> <div style="text-align: center;"> $\nabla f_{BP}(x) = H^T(HH^T + \epsilon I)^{-1}(Hx - y)$ <br><br> </div> <p class="text-justify">This expression is precisely the form of a <strong>Wiener filter</strong>. In the Fourier domain, if $\mathcal{F}(\cdot)$ is the Fourier transform, the operator $H^T(HH^T + \epsilon I)^{-1}$ becomes a filter with the frequency response:</p> <div style="text-align: center;"> $\displaystyle\frac{\overline{\mathcal{F}(h)}}{|\mathcal{F}(h)|^2 + \epsilon}$, <br><br> </div> <p class="text-justify">where $h$ is the blur kernel. This filter adaptively de-emphasizes frequencies that were attenuated by the blur, preventing noise amplification. Therefore, using the back-projection loss in a convolutional setting is akin to incorporating a Wiener-filter-like deconvolution step directly into the gradient update of the optimization, leading to more stable and accurate reconstructions.</p> <h2 id="solving-inverse-problems-with-the-method-of-alternating-proximations">Solving Inverse Problems with the Method of Alternating Proximations</h2> <p>This optimization problem can be solved efficiently using the <strong>proximal gradient method (PGM)</strong>. PGM is an iterative algorithm designed for problems that are a sum of a smooth function and a (possibly non-smooth) function for which we can compute a proximal operator.</p> <p>In our case, the objective function is $f(x) + h(x)$, where $f(x) = \frac{1}{2} \Vert A^{\dagger}(y - Ax)\Vert_2^2$ is the smooth data-fidelity term, and $h(x) = \lambda g(x)$ is the regularization term.</p> <p>The gradient of the smooth part is:</p> <div style="text-align: center;"> $\nabla f(x) = A^T(AA^T)^{-1}(Ax-y) = A^{\dagger}(Ax-y)$ <br><br> </div> <p>The PGM update step is given by:</p> <div style="text-align: center;"> $x_{k+1} = \text{prox}_{h}(x_k - \nabla f(x_k))$, <br> $x_{k+1} = \text{prox}_{\lambda g}(x_k - A^{\dagger}(Ax_k-y))$. <br><br> </div> <p>Notice that the term inside the proximal operator is exactly the projection of $x_k$ onto the solution space $C$:</p> <div style="text-align: center;"> $x_k - A^{\dagger}(Ax_k-y) = \text{proj}_C(x_k)$ <br><br> </div> <p>This allows us to write the update in a very intuitive form:</p> <div style="text-align: center;"> $x_{k+1} = \text{prox}_{\lambda g}(\text{prox}_{\iota_C}(x_k))$ <br><br> </div> <p class="text-justify">The iterative scheme that follows the proximal gradient method can also be viewed from the perspective of <strong>alternating approximations</strong> <d-cite key="kamath2024method"></d-cite>.</p> <p>Empirically, it was observed that the method of alternating proximations provided faster and superior-quality reconstructions compared to the standard least-squares data-fidelity for compressive sensing <d-cite key="nareddy2024tight"></d-cite> and image deconvolution <d-cite key="nareddy2024image"></d-cite>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-08-06-tight.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Spectrum Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/calgaryml/calgaryml.github.io" rel="external nofollow noopener" target="_blank">customized theme</a> based on <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>